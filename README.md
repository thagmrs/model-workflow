
# ğŸ§  Property Price Predictor â€“ Full MLOps Pipeline (AWS + Terraform + Docker + GitHub Actions)

<p align="center">
  <img src="https://img.shields.io/badge/AWS-S3%20%7C%20SNS%20%7C%20Lambda-orange?logo=amazon-aws"/>
  <img src="https://img.shields.io/badge/ECS-Fargate-blue?logo=amazon-ecs"/>
  <img src="https://img.shields.io/badge/IaC-Terraform%20v1.6%2B-blueviolet?logo=terraform"/>
  <img src="https://img.shields.io/badge/CI%2FCD-GitHub%20Actions%20âœ”ï¸-blue?logo=githubactions"/>
  <img src="https://img.shields.io/badge/Docker-Train%20%26%20Inference%20Images-gray?logo=docker"/>
</p>

---

## ğŸ“‹ Overview

This project implements a **fully automated MLOps pipeline** responsible for training, retraining, and serving Machine Learning models on **AWS**, combining:

- **S3 + SNS + Lambda** â†’ event-driven training/retraining automation  
- **ECR + ECS Fargate** â†’ containerized inference API hosting  
- **Terraform** â†’ full infrastructure provisioning (IaC)  
- **GitHub Actions** â†’ CI/CD, build, and deploy automation  
- **FastAPI** â†’ model inference serving with built-in OpenAPI documentation  

The model predicts **real estate prices** based on property type, area, location, and room count, using a `GradientBoostingRegressor` (Scikit-Learn).

---

## âš™ï¸ Solution Architecture

<p align="center">
  <img src="docs/arquit.png" alt="MLOps pipeline architecture" width="850"/>
</p>

---

## ğŸ§© End-to-End Workflow

1ï¸âƒ£ **Data ingestion**  
   - New data uploaded to the S3 bucket automatically triggers an event.

2ï¸âƒ£ **Event trigger (SNS + Lambda)**  
   - The event is published via **SNS**, which triggers the **Lambda Trainer** function.

3ï¸âƒ£ **Lambda Trainer (train/retrain)**  
   - The Lambda consumes the event, processes the dataset, and executes model training.  
   - The new artifact is saved to `s3://<artifacts-bucket>/models/latest/model.joblib`.

4ï¸âƒ£ **Inference API (ECS + FastAPI)**  
   - The container hosted on **ECS Fargate** exposes `/predict`, `/health`, `/docs`, and `/openapi.json`.  
   - The API authenticates via API Key and uses smart caching â€” it checks if the cached model is up to date and reloads only if a newer version exists.

5ï¸âƒ£ **CI/CD & Infrastructure (Terraform + GitHub Actions)**  
   - On every push to `main`:  
     - Docker images for training and inference are built and pushed to ECR,  
     - `terraform plan` and `apply` (or `destroy`) are executed,  
     - the full infrastructure is deployed and endpoints are outputed.

---

## ğŸ§± Folder Structure

```
.
â”œâ”€â”€ .github/workflows/deploy.yml
â”œâ”€â”€ config/pipeline_config.yml
â”œâ”€â”€ docs/arquitetura.png
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ alb.tf
â”‚   â”œâ”€â”€ apigw_ecs_proxy.tf
â”‚   â”œâ”€â”€ backend.tf
â”‚   â”œâ”€â”€ ecs.tf
â”‚   â”œâ”€â”€ iam.tf
â”‚   â”œâ”€â”€ lambda.tf
â”‚   â”œâ”€â”€ main.tf
â”‚   â”œâ”€â”€ network.tf
â”‚   â”œâ”€â”€ outputs.tf
â”‚   â”œâ”€â”€ s3.tf
â”‚   â”œâ”€â”€ security_groups.tf
â”‚   â””â”€â”€ variables.tf
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â”œâ”€â”€ predict.py
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ Dockerfile
â”‚       â”œâ”€â”€ requirements.txt
â”‚       â””â”€â”€ training.py
â””â”€â”€ README.md
```

---

## ğŸ” Automatic Retraining Flow

| Event | Action | Result |
|--------|---------|----------|
| New CSV uploaded to S3 | SNS publishes notification | Lambda is triggered |
| Lambda runs `train.py` | Trains & evaluates model | Saves updated `model.joblib` to S3 |
| ECS API detects new model | Updates local cache | Serves predictions using the latest model |

---

## ğŸ§  Inference API

| Method | Route | Description |
|---------|-------|-------------|
| `POST` | `/predict` | Predicts price based on input features |
| `GET` | `/health` | API status and model info |
| `GET` | `/docs` | Auto-generated Documentation |
| `GET` | `/openapi.json` | OpenAPI schema |

### Example request

```bash
curl -X POST "https://<api_id>.execute-api.us-east-2.amazonaws.com/prod/predict"   -H "Content-Type: application/json"   -H "x-api-key: <api_key>"   -d '{
    "type": "house",
    "sector": "las condes",
    "net_usable_area": 110.0,
    "net_area": 200.0,
    "n_rooms": 3,
    "n_bathroom": 3,
    "latitude": -33.3932,
    "longitude": -70.5505
  }'
```

---

## âš™ï¸ Requirements

| Tool | Minimum Version | Purpose |
|-------|------------------|----------|
| Terraform | â‰¥ 1.6 | Infrastructure provisioning |
| AWS CLI | â‰¥ 2.15 | AWS interaction |
| Docker | â‰¥ 24.0 | Build images |
| Python | â‰¥ 3.10 | Training scripts |
| GitHub Actions | Built-in | CI/CD |

---

## ğŸ”‘ Required Configurations

### ğŸ” GitHub Secrets

| Name | Description |
|------|--------------|
| `AWS_ACCESS_KEY_ID` | AWS access key |
| `AWS_SECRET_ACCESS_KEY` | AWS secret key |
| `AWS_REGION` | AWS region |

---

## ğŸ§± Remote Terraform Backend

Create the following resources on AWS:

- **S3 Terraform Bucket** â†’ stores `terraform.tfstate`  
- **S3 Data Bucket** â†’ where your data for training is stored
- **DynamoDB Table** â†’ manages Terraform state locks  

```hcl
terraform {
  backend "s3" {
    bucket         = BUCKET_NAME
    key            = "terraform/state.tfstate"
    region         = var.aws_region
    dynamodb_table = DYNAMO_TABLE_NAME
    encrypt        = true
  }
}
```

Set variables in your `.tfvars` file matching `variables.tf`.

---

## âš™ï¸ Workflow Config (GitHub Actions)

File: `config/pipeline_config.yml`

| Key | Description |
|------|--------------|
| `aws_account_id` | AWS account ID |
| `aws_region` | AWS region |
| `ecr_training_repo` | ECR repository for lambda training image |
| `ecr_inference_repo` | ECR repository for ECS inference image |
| `terraform_state_bucket` | S3 bucket for Terraform state |
| `terraform_lock_table` | DynamoDB table for Terraform locks |
| `project_name` | Project name |
| `action` | Terraform action: `apply` or `destroy` |

---

## âœ¨ Author

Developed by **ThaÃ­s GuimarÃ£es (thagmrs)** ğŸ¦«  
ğŸ’¼ ML & Data Engineering  
ğŸ”— [linkedin.com/in/thaisgmrs](https://linkedin.com/in/thaisgmrs)

---

## ğŸ”® Future Enhancements

- Add a data validation step before training  
- Make data ingestion source-agnostic (as long as required columns exist)  
- Implement model versioning (e.g., MLflow or Model Registry)  
- Log model metrics and retrain only if performance degrades  
- Migrate training to AWS Batch or other scalable compute services  
- Replace API Key with Cognito Authorizer  
- Create `staging` and `prod` environments via Terraform workspaces  
- Enhance GitHub Actions workflow with tests and multi-step validation  
